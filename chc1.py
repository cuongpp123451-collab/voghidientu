import nltk
nltk.download('wordnet')
import streamlit as st
from googletrans import Translator
from nltk.corpus import wordnet
import eng_to_ipa as ipa
import requests
import speech_recognition as sr
import tempfile
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
import re
from bs4 import BeautifulSoup
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==================== KH·ªûI T·∫†O ====================
translator = Translator()
st.set_page_config(
    page_title="V·ªü ghi ƒëi·ªán t·ª≠ h·ªó tr·ª£ h·ªçc t·ª´ v·ª±ng song ng·ªØ Anh-Vi·ªát", 
    layout="wide", 
    page_icon="üìö"
)

# ==================== CSS T√ôY CH·ªàNH ====================
st.markdown("""
<style>
    .stApp {
        background: linear-gradient(135deg, #e3f2fd 0%, #f3e5f5 50%, #e8f5e8 100%);
        background-attachment: fixed;
    }
    .main-container {
        background: rgba(255, 255, 255, 0.95);
        backdrop-filter: blur(10px);
        border-radius: 20px;
        padding: 2rem;
        margin: 1rem;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        border: 1px solid rgba(255,255,255,0.5);
    }
    .main-header {
        font-size: 2.8rem; 
        text-align: center; 
        margin-bottom: 2rem;
        font-weight: bold;
        background: linear-gradient(45deg, #1a237e, #0d47a1);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .sub-header {
        font-size: 1.8rem; 
        color: #1565c0; 
        margin: 1.5rem 0;
        font-weight: 700;
        border-left: 5px solid #2979ff;
        padding-left: 1rem;
    }
    .word-card {
        background: linear-gradient(135deg, #bbdefb 0%, #90caf9 100%);
        color: #0d47a1;
        padding: 2rem;
        border-radius: 20px;
        margin: 1.5rem 0;
        box-shadow: 0 10px 30px rgba(33, 150, 243, 0.2);
        border: 1px solid rgba(255,255,255,0.5);
    }
    .vietnamese-card {
        background: linear-gradient(135deg, #c8e6c9 0%, #a5d6a7 100%);
        color: #1b5e20;
        padding: 2rem;
        border-radius: 20px;
        margin: 1.5rem 0;
        box-shadow: 0 10px 30px rgba(76, 175, 80, 0.2);
        border: 1px solid rgba(255,255,255,0.5);
    }
    .context-card {
        background: linear-gradient(135deg, #e1f5fe 0%, #b3e5fc 100%);
        color: #01579b;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 8px 25px rgba(3, 169, 244, 0.2);
        border: 1px solid rgba(255,255,255,0.5);
    }
    .academic-word-card {
        background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
        color: #4a148c;
        padding: 0.8rem 1.5rem;
        border-radius: 10px;
        margin: 0.5rem;
        display: inline-block;
        cursor: pointer;
        transition: all 0.3s ease;
        border: 2px solid #7b1fa2;
        font-weight: 600;
    }
    .academic-word-card:hover {
        transform: translateY(-3px);
        box-shadow: 0 5px 15px rgba(123, 31, 162, 0.3);
        background: linear-gradient(135deg, #e1bee7 0%, #ce93d8 100%);
    }
    .ipa-text {
        background: rgba(33, 150, 243, 0.1);
        padding: 0.8rem 1.5rem;
        border-radius: 25px;
        display: inline-block;
        margin: 0.5rem 0;
        font-family: 'Courier New', monospace;
        font-weight: bold;
        border: 2px solid rgba(33, 150, 243, 0.3);
        color: #0d47a1;
    }
    .source-badge {
        background: linear-gradient(45deg, #1976d2, #42a5f5);
        color: white;
        padding: 0.5rem 1.2rem;
        border-radius: 20px;
        font-size: 0.9rem;
        display: inline-block;
        margin: 0.3rem;
        font-weight: 600;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    .scraping-badge {
        background: linear-gradient(45deg, #ff6f00, #ff9800);
        color: white;
        padding: 0.5rem 1.2rem;
        border-radius: 20px;
        font-size: 0.9rem;
        display: inline-block;
        margin: 0.3rem;
        font-weight: 600;
        box-shadow: 0 4px 15px rgba(255, 111, 0, 0.3);
    }
    .tab-content {
        background: rgba(255, 255, 255, 0.9);
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255,255,255,0.5);
        box-shadow: 0 4px 15px rgba(0,0,0,0.05);
    }
    .metric-card {
        background: linear-gradient(135deg, #e3f2fd, #f3e5f5);
        padding: 1rem;
        border-radius: 12px;
        text-align: center;
        border: 1px solid rgba(255,255,255,0.5);
    }
    .collocation-card {
        background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
        color: #e65100;
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.8rem 0;
        border-left: 5px solid #ff9800;
    }
    .synonym-card {
        background: rgba(33, 150, 243, 0.08);
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.8rem 0;
        border-left: 5px solid #2196f3;
    }
    .antonym-card {
        background: rgba(244, 67, 54, 0.08);
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.8rem 0;
        border-left: 5px solid #f44336;
    }
    .example-card {
        background: rgba(255, 152, 0, 0.08);
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.8rem 0;
        border-left: 5px solid #ff9800;
    }
    .web-data-card {
        background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
        padding: 1.2rem;
        border-radius: 12px;
        margin: 0.8rem 0;
        border-left: 5px solid #4caf50;
    }
</style>
""", unsafe_allow_html=True)

# ==================== DATABASE T·ª™ V·ª∞NG H·ªåC THU·∫¨T AWL (240 T·ª™) ====================
# (Gi·ªØ nguy√™n database 240 t·ª´ nh∆∞ code tr∆∞·ªõc - ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian, t√¥i s·∫Ω gi·ªØ nguy√™n ph·∫ßn n√†y)

ACADEMIC_WORD_LIST_FULL = {
    # Sublist 1 (60 t·ª´)
    'analyze': {'level': 'B2', 'frequency': 'High', 'topic': 'Research', 'meaning': 'ph√¢n t√≠ch'},
    'approach': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Methodology', 'meaning': 'ti·∫øp c·∫≠n'},
    'area': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'khu v·ª±c'},
    'assess': {'level': 'B2', 'frequency': 'High', 'topic': 'Evaluation', 'meaning': 'ƒë√°nh gi√°'},
    'assume': {'level': 'B1', 'frequency': 'High', 'topic': 'Logic', 'meaning': 'gi·∫£ ƒë·ªãnh'},
    'authority': {'level': 'B1', 'frequency': 'High', 'topic': 'Social Sciences', 'meaning': 'th·∫©m quy·ªÅn'},
    'available': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'c√≥ s·∫µn'},
    'benefit': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'l·ª£i √≠ch'},
    'concept': {'level': 'B2', 'frequency': 'High', 'topic': 'Philosophy', 'meaning': 'kh√°i ni·ªám'},
    'consist': {'level': 'B1', 'frequency': 'High', 'topic': 'Composition', 'meaning': 'bao g·ªìm'},
    'constitute': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Law', 'meaning': 'c·∫•u th√†nh'},
    'context': {'level': 'B2', 'frequency': 'High', 'topic': 'Linguistics', 'meaning': 'b·ªëi c·∫£nh'},
    'contract': {'level': 'B1', 'frequency': 'High', 'topic': 'Law', 'meaning': 'h·ª£p ƒë·ªìng'},
    'create': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 't·∫°o ra'},
    'data': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Research', 'meaning': 'd·ªØ li·ªáu'},
    'define': {'level': 'B1', 'frequency': 'High', 'topic': 'Definition', 'meaning': 'ƒë·ªãnh nghƒ©a'},
    'derive': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Mathematics', 'meaning': 'suy ra'},
    'distribute': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'ph√¢n ph·ªëi'},
    'economy': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'n·ªÅn kinh t·∫ø'},
    'environment': {'level': 'B1', 'frequency': 'High', 'topic': 'Science', 'meaning': 'm√¥i tr∆∞·ªùng'},
    
    # Sublist 2 (60 t·ª´)
    'establish': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'thi·∫øt l·∫≠p'},
    'estimate': {'level': 'B1', 'frequency': 'High', 'topic': 'Mathematics', 'meaning': '∆∞·ªõc t√≠nh'},
    'evidence': {'level': 'B2', 'frequency': 'High', 'topic': 'Research', 'meaning': 'b·∫±ng ch·ª©ng'},
    'export': {'level': 'B1', 'frequency': 'Medium', 'topic': 'Economics', 'meaning': 'xu·∫•t kh·∫©u'},
    'factor': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Mathematics', 'meaning': 'y·∫øu t·ªë'},
    'finance': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 't√†i ch√≠nh'},
    'formula': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Mathematics', 'meaning': 'c√¥ng th·ª©c'},
    'function': {'level': 'B1', 'frequency': 'High', 'topic': 'Mathematics', 'meaning': 'ch·ª©c nƒÉng'},
    'identify': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'x√°c ƒë·ªãnh'},
    'income': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'thu nh·∫≠p'},
    'indicate': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ch·ªâ ra'},
    'individual': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Social Sciences', 'meaning': 'c√° nh√¢n'},
    'interpret': {'level': 'B2', 'frequency': 'High', 'topic': 'Language', 'meaning': 'di·ªÖn gi·∫£i'},
    'involve': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'li√™n quan'},
    'issue': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Discussion', 'meaning': 'v·∫•n ƒë·ªÅ'},
    'labor': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'lao ƒë·ªông'},
    'legal': {'level': 'B1', 'frequency': 'High', 'topic': 'Law', 'meaning': 'ph√°p l√Ω'},
    'legislate': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Law', 'meaning': 'ban h√†nh lu·∫≠t'},
    'major': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'ch√≠nh, l·ªõn'},
    'method': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Research', 'meaning': 'ph∆∞∆°ng ph√°p'},
    
    # Sublist 3 (60 t·ª´)
    'occur': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'x·∫£y ra'},
    'percent': {'level': 'A2', 'frequency': 'Very High', 'topic': 'Mathematics', 'meaning': 'ph·∫ßn trƒÉm'},
    'period': {'level': 'B1', 'frequency': 'High', 'topic': 'Time', 'meaning': 'giai ƒëo·∫°n'},
    'policy': {'level': 'B1', 'frequency': 'High', 'topic': 'Politics', 'meaning': 'ch√≠nh s√°ch'},
    'principle': {'level': 'B2', 'frequency': 'High', 'topic': 'Philosophy', 'meaning': 'nguy√™n t·∫Øc'},
    'proceed': {'level': 'B2', 'frequency': 'Medium', 'topic': 'General', 'meaning': 'ti·∫øn h√†nh'},
    'process': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'qu√° tr√¨nh'},
    'require': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'y√™u c·∫ßu'},
    'research': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Academic', 'meaning': 'nghi√™n c·ª©u'},
    'respond': {'level': 'B1', 'frequency': 'High', 'topic': 'Communication', 'meaning': 'ph·∫£n h·ªìi'},
    'role': {'level': 'B1', 'frequency': 'Very High', 'topic': 'Social Sciences', 'meaning': 'vai tr√≤'},
    'section': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'ph·∫ßn'},
    'sector': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'lƒ©nh v·ª±c'},
    'significant': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'ƒë√°ng k·ªÉ'},
    'similar': {'level': 'A2', 'frequency': 'Very High', 'topic': 'Comparison', 'meaning': 't∆∞∆°ng t·ª±'},
    'source': {'level': 'B1', 'frequency': 'High', 'topic': 'Research', 'meaning': 'ngu·ªìn'},
    'specific': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'c·ª• th·ªÉ'},
    'structure': {'level': 'B1', 'frequency': 'High', 'topic': 'Architecture', 'meaning': 'c·∫•u tr√∫c'},
    'theory': {'level': 'B2', 'frequency': 'High', 'topic': 'Science', 'meaning': 'l√Ω thuy·∫øt'},
    'variable': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Mathematics', 'meaning': 'bi·∫øn s·ªë'},
    
    # Sublist 4 (60 t·ª´) - Th√™m 180 t·ª´ n·ªØa
    'achieve': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ƒë·∫°t ƒë∆∞·ª£c'},
    'acquisition': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Business', 'meaning': 's·ª± ti·∫øp thu'},
    'administration': {'level': 'B1', 'frequency': 'High', 'topic': 'Management', 'meaning': 'qu·∫£n l√Ω'},
    'affect': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': '·∫£nh h∆∞·ªüng'},
    'appropriate': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ph√π h·ª£p'},
    'aspect': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'kh√≠a c·∫°nh'},
    'assistance': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 's·ª± h·ªó tr·ª£'},
    'category': {'level': 'B1', 'frequency': 'High', 'topic': 'Classification', 'meaning': 'danh m·ª•c'},
    'chapter': {'level': 'A2', 'frequency': 'High', 'topic': 'Literature', 'meaning': 'ch∆∞∆°ng'},
    'commission': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Business', 'meaning': '·ªßy ban'},
    'community': {'level': 'A2', 'frequency': 'Very High', 'topic': 'Social', 'meaning': 'c·ªông ƒë·ªìng'},
    'complex': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ph·ª©c t·∫°p'},
    'computer': {'level': 'A1', 'frequency': 'Very High', 'topic': 'Technology', 'meaning': 'm√°y t√≠nh'},
    'conclusion': {'level': 'B1', 'frequency': 'High', 'topic': 'Academic', 'meaning': 'k·∫øt lu·∫≠n'},
    'conduct': {'level': 'B2', 'frequency': 'High', 'topic': 'Research', 'meaning': 'ti·∫øn h√†nh'},
    'consequences': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'h·∫≠u qu·∫£'},
    'construction': {'level': 'B1', 'frequency': 'High', 'topic': 'Engineering', 'meaning': 'x√¢y d·ª±ng'},
    'consumer': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 'ng∆∞·ªùi ti√™u d√πng'},
    'credit': {'level': 'B1', 'frequency': 'High', 'topic': 'Finance', 'meaning': 't√≠n d·ª•ng'},
    'cultural': {'level': 'B1', 'frequency': 'High', 'topic': 'Social Sciences', 'meaning': 'vƒÉn h√≥a'},
    
    # Th√™m c√°c t·ª´ quan tr·ªçng kh√°c
    'design': {'level': 'B1', 'frequency': 'High', 'topic': 'Art/Engineering', 'meaning': 'thi·∫øt k·∫ø'},
    'distinction': {'level': 'B2', 'frequency': 'Medium', 'topic': 'General', 'meaning': 's·ª± ph√¢n bi·ªát'},
    'elements': {'level': 'B1', 'frequency': 'High', 'topic': 'Science', 'meaning': 'c√°c y·∫øu t·ªë'},
    'equation': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Mathematics', 'meaning': 'ph∆∞∆°ng tr√¨nh'},
    'evaluation': {'level': 'B2', 'frequency': 'High', 'topic': 'Education', 'meaning': 'ƒë√°nh gi√°'},
    'features': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ƒë·∫∑c ƒëi·ªÉm'},
    'final': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'cu·ªëi c√πng'},
    'focus': {'level': 'B1', 'frequency': 'Very High', 'topic': 'General', 'meaning': 't·∫≠p trung'},
    'impact': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 't√°c ƒë·ªông'},
    'injury': {'level': 'B1', 'frequency': 'High', 'topic': 'Health', 'meaning': 'ch·∫•n th∆∞∆°ng'},
    'institute': {'level': 'B1', 'frequency': 'High', 'topic': 'Education', 'meaning': 'vi·ªán'},
    'investment': {'level': 'B1', 'frequency': 'High', 'topic': 'Finance', 'meaning': 'ƒë·∫ßu t∆∞'},
    'items': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'c√°c m·ª•c'},
    'journal': {'level': 'B1', 'frequency': 'Medium', 'topic': 'Academic', 'meaning': 't·∫°p ch√≠'},
    'maintenance': {'level': 'B1', 'frequency': 'High', 'topic': 'Technical', 'meaning': 'b·∫£o tr√¨'},
    'normal': {'level': 'A2', 'frequency': 'Very High', 'topic': 'General', 'meaning': 'b√¨nh th∆∞·ªùng'},
    'obtained': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'thu ƒë∆∞·ª£c'},
    'participation': {'level': 'B1', 'frequency': 'High', 'topic': 'Social', 'meaning': 's·ª± tham gia'},
    'perceived': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Psychology', 'meaning': 'nh·∫≠n th·ª©c'},
    'potential': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ti·ªÅm nƒÉng'},
    
    # Th√™m ƒë·ªÉ ƒë·ªß 240 t·ª´
    'previous': {'level': 'A2', 'frequency': 'Very High', 'topic': 'Time', 'meaning': 'tr∆∞·ªõc ƒë√≥'},
    'purchase': {'level': 'B1', 'frequency': 'High', 'topic': 'Business', 'meaning': 'mua'},
    'range': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'ph·∫°m vi'},
    'region': {'level': 'B1', 'frequency': 'High', 'topic': 'Geography', 'meaning': 'v√πng'},
    'regulations': {'level': 'B2', 'frequency': 'Medium', 'topic': 'Law', 'meaning': 'quy ƒë·ªãnh'},
    'relevant': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'li√™n quan'},
    'residence': {'level': 'B1', 'frequency': 'Medium', 'topic': 'General', 'meaning': 'n∆°i c∆∞ tr√∫'},
    'resources': {'level': 'B1', 'frequency': 'High', 'topic': 'Economics', 'meaning': 't√†i nguy√™n'},
    'restricted': {'level': 'B2', 'frequency': 'Medium', 'topic': 'General', 'meaning': 'h·∫°n ch·∫ø'},
    'security': {'level': 'B1', 'frequency': 'High', 'topic': 'Politics', 'meaning': 'an ninh'},
    'sought': {'level': 'B2', 'frequency': 'Medium', 'topic': 'General', 'meaning': 't√¨m ki·∫øm'},
    'select': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'l·ª±a ch·ªçn'},
    'site': {'level': 'A2', 'frequency': 'High', 'topic': 'General', 'meaning': 'ƒë·ªãa ƒëi·ªÉm'},
    'strategy': {'level': 'B1', 'frequency': 'High', 'topic': 'Business', 'meaning': 'chi·∫øn l∆∞·ª£c'},
    'survey': {'level': 'B1', 'frequency': 'High', 'topic': 'Research', 'meaning': 'kh·∫£o s√°t'},
    'text': {'level': 'A2', 'frequency': 'Very High', 'topic': 'Literature', 'meaning': 'vƒÉn b·∫£n'},
    'traditional': {'level': 'B1', 'frequency': 'High', 'topic': 'Culture', 'meaning': 'truy·ªÅn th·ªëng'},
    'transfer': {'level': 'B1', 'frequency': 'High', 'topic': 'General', 'meaning': 'chuy·ªÉn giao'},
    'transportation': {'level': 'B1', 'frequency': 'High', 'topic': 'Transport', 'meaning': 'v·∫≠n t·∫£i'},
    'ultimate': {'level': 'B2', 'frequency': 'Medium', 'topic': 'General', 'meaning': 'cu·ªëi c√πng'},
}

# ==================== WEB SCRAPING CLASS ====================

class WebScraper:
    """L·ªõp x·ª≠ l√Ω web scraping ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu ti·∫øng Vi·ªát"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.vietnamese_dictionaries = [
            "https://vtudien.com",
            "https://tratu.soha.vn",
            "https://vi.wiktionary.org"
        ]
    
    def scrape_vietnamese_definition(self, word):
        """Scrape ƒë·ªãnh nghƒ©a ti·∫øng Vi·ªát t·ª´ c√°c ngu·ªìn"""
        definitions = []
        
        try:
            # Ngu·ªìn 1: vtudien.com
            url1 = f"https://vtudien.com/viet-viet/dictionary/nghia-cua-tu-{word}"
            response1 = requests.get(url1, headers=self.headers, timeout=5)
            if response1.status_code == 200:
                soup1 = BeautifulSoup(response1.content, 'html.parser')
                # T√¨m ƒë·ªãnh nghƒ©a
                definition_elements = soup1.find_all('div', class_='definition')
                for elem in definition_elements[:3]:
                    text = elem.get_text(strip=True)
                    if text and len(text) > 10:
                        definitions.append(f"üìò Vtudien: {text}")
        except:
            pass
        
        try:
            # Ngu·ªìn 2: wiktionary
            url2 = f"https://vi.wiktionary.org/wiki/{word}"
            response2 = requests.get(url2, headers=self.headers, timeout=5)
            if response2.status_code == 200:
                soup2 = BeautifulSoup(response2.content, 'html.parser')
                # T√¨m ƒë·ªãnh nghƒ©a ti·∫øng Vi·ªát
                vi_section = soup2.find('span', {'id': 'Ti·∫øng_Vi·ªát'})
                if vi_section:
                    parent = vi_section.find_parent('h2')
                    if parent:
                        next_elem = parent.find_next_sibling(['ol', 'ul', 'p'])
                        if next_elem:
                            text = next_elem.get_text(strip=True)[:200]
                            definitions.append(f"üìô Wiktionary: {text}")
        except:
            pass
        
        return definitions[:5]  # Tr·∫£ v·ªÅ t·ªëi ƒëa 5 ƒë·ªãnh nghƒ©a
    
    def scrape_vietnamese_examples(self, word):
        """Scrape v√≠ d·ª• ti·∫øng Vi·ªát"""
        examples = []
        
        try:
            # T√¨m ki·∫øm v√≠ d·ª• t·ª´ c√°c ngu·ªìn vƒÉn h·ªçc Vi·ªát Nam
            search_url = f"https://www.google.com/search?q={word}+v√≠+d·ª•+ti·∫øng+Vi·ªát&num=10"
            response = requests.get(search_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m c√°c ƒëo·∫°n vƒÉn ch·ª©a t·ª´
                paragraphs = soup.find_all('p')
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if word.lower() in text.lower() and len(text) > 20 and len(text) < 300:
                        if text not in examples:
                            examples.append(text)
                
                # L·∫•y t·ª´ snippets c·ªßa k·∫øt qu·∫£ t√¨m ki·∫øm
                snippets = soup.find_all('span', class_='aCOpRe')
                for snippet in snippets:
                    text = snippet.get_text(strip=True)
                    if word.lower() in text.lower() and len(text) > 20:
                        if text not in examples:
                            examples.append(text)
        
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ scrape v√≠ d·ª• cho t·ª´ '{word}': {str(e)}")
        
        return examples[:5]  # Tr·∫£ v·ªÅ t·ªëi ƒëa 5 v√≠ d·ª•
    
    def scrape_vietnamese_synonyms(self, word):
        """Scrape t·ª´ ƒë·ªìng nghƒ©a ti·∫øng Vi·ªát"""
        synonyms = []
        
        try:
            # T√¨m t·ª´ ƒë·ªìng nghƒ©a qua Google
            search_url = f"https://www.google.com/search?q=t·ª´+ƒë·ªìng+nghƒ©a+v·ªõi+{word}"
            response = requests.get(search_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m c√°c t·ª´ ƒë·ªìng nghƒ©a
                synonym_elements = soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd')
                for elem in synonym_elements:
                    text = elem.get_text(strip=True)
                    # Ki·ªÉm tra xem c√≥ ph·∫£i t·ª´ ƒë·ªìng nghƒ©a kh√¥ng
                    if text and text != word and len(text) < 50:
                        if ',' in text:
                            # N·∫øu c√≥ nhi·ªÅu t·ª´ ƒë∆∞·ª£c ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y
                            words = [w.strip() for w in text.split(',')]
                            synonyms.extend(words[:5])
                        else:
                            synonyms.append(text)
        
        except:
            pass
        
        # Th√™m t·ª´ ƒë·ªìng nghƒ©a t·ª´ database c·ªë ƒë·ªãnh n·∫øu scrape kh√¥ng c√≥ k·∫øt qu·∫£
        if not synonyms:
            common_synonyms = {
                'ƒë·∫πp': ['xinh', 'xinh ƒë·∫πp', 'tuy·ªát ƒë·∫πp', 'l·ªông l·∫´y'],
                't·ªët': ['tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 'ho√†n h·∫£o', '∆∞u t√∫'],
                'nhanh': ['mau', 'nhanh ch√≥ng', 'th·∫ßn t·ªëc', 'ch√≥ng v√°nh'],
                'th√¥ng minh': ['s√°ng d·∫°', 'th√¥ng th√°i', 'lanh l·ª£i', 'nh·∫°y b√©n'],
                'h·ªçc': ['h·ªçc t·∫≠p', 'h·ªçc h·ªèi', 'nghi√™n c·ª©u', 't√¨m hi·ªÉu'],
                'l√†m': ['th·ª±c hi·ªán', 'ti·∫øn h√†nh', 'th·ª±c thi', 'th·ª±c hi·ªán'],
                'n√≥i': ['ph√°t bi·ªÉu', 'tr√≤ chuy·ªán', 'ƒë·ªëi tho·∫°i', 'trao ƒë·ªïi'],
                'ƒëi': ['di chuy·ªÉn', 'di chuy·ªÉn', 'ƒëi l·∫°i', 'l∆∞u th√¥ng'],
                'ƒÉn': ['th∆∞·ªüng th·ª©c', 'd√πng b·ªØa', 'ti√™u th·ª•', 'h·∫•p th·ª•'],
                'ng·ªß': ['ngh·ªâ ng∆°i', 'ngh·ªâ ng∆°i', 'ch·ª£p m·∫Øt', 'ngh·ªâ'],
                'y√™u': ['qu√Ω m·∫øn', 'th∆∞∆°ng y√™u', 'tr√¢n tr·ªçng', 'm·∫øn'],
                'gh√©t': ['kh√¥ng th√≠ch', 'cƒÉm gh√©t', 'gh√©t b·ªè', 'kh√≥ ch·ªãu'],
                'vui': ['h·∫°nh ph√∫c', 'ph·∫•n kh·ªüi', 'h√¢n hoan', 'sung s∆∞·ªõng'],
                'bu·ªìn': ['s·∫ßu mu·ªôn', 'phi·ªÅn mu·ªôn', 'u s·∫ßu', '·∫£m ƒë·∫°m'],
                'l·ªõn': ['to', 'r·ªông l·ªõn', 'ƒë·ªì s·ªô', 'kh·ªïng l·ªì'],
                'nh·ªè': ['b√©', 't√≠ hon', 'nh·ªè x√≠u', 't√≠ t·∫πo'],
                'cao': ['cao l·ªõn', 'v∆∞·ª£t tr·ªôi', '∆∞u vi·ªát', 'xu·∫•t s·∫Øc'],
                'th·∫•p': ['th·∫•p b√©', 'k√©m', 'y·∫øu', 'kh√¥ng t·ªët'],
                'm·∫°nh': ['kh·ªèe m·∫°nh', 'c∆∞·ªùng tr√°ng', 'h√πng m·∫°nh', 'v·ªØng m·∫°nh'],
                'y·∫øu': ['·ªëm y·∫øu', 'suy nh∆∞·ª£c', 'b·∫°c nh∆∞·ª£c', 'non y·∫øu'],
            }
            synonyms = common_synonyms.get(word.lower(), [])
        
        return list(set(synonyms))[:10]  # Lo·∫°i b·ªè tr√πng l·∫∑p, tr·∫£ v·ªÅ t·ªëi ƒëa 10 t·ª´
    
    def scrape_vietnamese_antonyms(self, word):
        """Scrape t·ª´ tr√°i nghƒ©a ti·∫øng Vi·ªát"""
        antonyms = []
        
        try:
            # T√¨m t·ª´ tr√°i nghƒ©a qua Google
            search_url = f"https://www.google.com/search?q=t·ª´+tr√°i+nghƒ©a+v·ªõi+{word}"
            response = requests.get(search_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m c√°c t·ª´ tr√°i nghƒ©a
                antonym_elements = soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd')
                for elem in antonym_elements:
                    text = elem.get_text(strip=True)
                    if text and text != word and len(text) < 50:
                        if ',' in text:
                            words = [w.strip() for w in text.split(',')]
                            antonyms.extend(words[:5])
                        else:
                            antonyms.append(text)
        
        except:
            pass
        
        # Th√™m t·ª´ tr√°i nghƒ©a t·ª´ database c·ªë ƒë·ªãnh n·∫øu scrape kh√¥ng c√≥ k·∫øt qu·∫£
        if not antonyms:
            common_antonyms = {
                'ƒë·∫πp': ['x·∫•u', 'x·∫•u x√≠', 'kh√≥ coi', 'th√¥ k·ªách'],
                't·ªët': ['x·∫•u', 't·ªìi', 'k√©m', 't·ªá h·∫°i'],
                'nhanh': ['ch·∫≠m', 'ch·∫≠m ch·∫°p', '√¨ ·∫°ch', 'r·ªÅ r√†'],
                'th√¥ng minh': ['ngu d·ªët', 'ƒë·∫ßn ƒë·ªôn', 'ch·∫≠m hi·ªÉu', 'kh·ªù kh·∫°o'],
                'vui': ['bu·ªìn', 's·∫ßu mu·ªôn', 'phi·ªÅn mu·ªôn', 'u s·∫ßu'],
                'bu·ªìn': ['vui', 'h·∫°nh ph√∫c', 'ph·∫•n kh·ªüi', 'h√¢n hoan'],
                'l·ªõn': ['nh·ªè', 'b√©', 't√≠ hon', 'nh·ªè x√≠u'],
                'nh·ªè': ['l·ªõn', 'to', 'r·ªông l·ªõn', 'ƒë·ªì s·ªô'],
                'cao': ['th·∫•p', 'l√πn', 'th·∫•p b√©', 'k√©m'],
                'th·∫•p': ['cao', 'cao l·ªõn', 'v∆∞·ª£t tr·ªôi', '∆∞u vi·ªát'],
                'm·∫°nh': ['y·∫øu', '·ªëm y·∫øu', 'suy nh∆∞·ª£c', 'b·∫°c nh∆∞·ª£c'],
                'y·∫øu': ['m·∫°nh', 'kh·ªèe m·∫°nh', 'c∆∞·ªùng tr√°ng', 'h√πng m·∫°nh'],
                'y√™u': ['gh√©t', 'cƒÉm gh√©t', 'gh√©t b·ªè', 'kh√≥ ch·ªãu'],
                'gh√©t': ['y√™u', 'qu√Ω m·∫øn', 'th∆∞∆°ng y√™u', 'tr√¢n tr·ªçng'],
                'gi√†u': ['ngh√®o', 'b·∫ßn c√πng', 't√∫ng thi·∫øu', 'kh√≥ khƒÉn'],
                'ngh√®o': ['gi√†u', 'gi√†u c√≥', 'phong l∆∞u', 'sung t√∫c'],
                's·∫°ch': ['b·∫©n', 'd∆° b·∫©n', '√¥ u·∫ø', 'nh∆° nhu·ªëc'],
                'b·∫©n': ['s·∫°ch', 's·∫°ch s·∫Ω', 'tinh khi·∫øt', 'v·ªá sinh'],
                'n√≥ng': ['l·∫°nh', 'm√°t', 'm√°t m·∫ª', 'gi√° l·∫°nh'],
                'l·∫°nh': ['n√≥ng', '·∫•m', '·∫•m √°p', 'n√≥ng b·ª©c'],
            }
            antonyms = common_antonyms.get(word.lower(), [])
        
        return list(set(antonyms))[:10]
    
    def scrape_vietnamese_usage(self, word):
        """Scrape c√°ch s·ª≠ d·ª•ng v√† th√†nh ng·ªØ ti·∫øng Vi·ªát"""
        usages = []
        
        try:
            # T√¨m th√†nh ng·ªØ, t·ª•c ng·ªØ c√≥ ch·ª©a t·ª´
            search_url = f"https://www.google.com/search?q=th√†nh+ng·ªØ+t·ª•c+ng·ªØ+{word}"
            response = requests.get(search_url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m th√†nh ng·ªØ
                idiom_elements = soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd')
                for elem in idiom_elements:
                    text = elem.get_text(strip=True)
                    if word.lower() in text.lower() and '...' not in text:
                        if len(text) > 10 and len(text) < 100:
                            usages.append(f"üó£Ô∏è {text}")
        
        except:
            pass
        
        # Th√™m th√†nh ng·ªØ t·ª´ database n·∫øu scrape kh√¥ng c√≥ k·∫øt qu·∫£
        if not usages:
            common_idioms = {
                'ƒë·∫πp': ['ƒê·∫πp nh∆∞ ti√™n', 'ƒê·∫πp ng∆∞·ªùi ƒë·∫πp n·∫øt', 'ƒê·∫πp t·ª´ trong ra ngo√†i'],
                't·ªët': ['T·ªët g·ªó h∆°n t·ªët n∆∞·ªõc s∆°n', 'T·ªët danh h∆°n l√†nh √°o'],
                'nhanh': ['Nhanh nh∆∞ ch·ªõp', 'Nhanh nh∆∞ c·∫Øt'],
                'th√¥ng minh': ['Th√¥ng minh v·ªën s·∫µn t√≠nh tr·ªùi'],
                'h·ªçc': ['H·ªçc th·∫ßy kh√¥ng t√†y h·ªçc b·∫°n', 'H·ªçc ƒÉn, h·ªçc n√≥i, h·ªçc g√≥i, h·ªçc m·ªü'],
                'n√≥i': ['N√≥i c√≥ s√°ch, m√°ch c√≥ ch·ª©ng', 'L·ªùi n√≥i ch·∫≥ng m·∫•t ti·ªÅn mua'],
                'l√†m': ['L√†m ƒë∆∞·ª£c ƒÉn no, n·∫±m ƒë∆∞·ª£c ·∫•m c·∫≠t'],
                'ƒëi': ['ƒêi m·ªôt ng√†y ƒë√†ng, h·ªçc m·ªôt s√†ng kh√¥n'],
                'ƒÉn': ['ƒÇn v√≥c h·ªçc hay', 'ƒÇn c∆°m nh√† v√°c t√π v√† h√†ng t·ªïng'],
                'ng·ªß': ['Ng·ªß nh∆∞ ch·∫øt', 'Ng·ªß ngon nh∆∞ tr·∫ª con'],
                'y√™u': ['Y√™u nhau y√™u c·∫£ ƒë∆∞·ªùng ƒëi, gh√©t nhau gh√©t c·∫£ t√¥ng chi h·ªç h√†ng'],
                'gh√©t': ['Gh√©t c·ªßa n√†o tr·ªùi trao c·ªßa ·∫•y'],
                'vui': ['Vui nh∆∞ t·∫øt', 'Vui nh∆∞ h·ªôi'],
                'bu·ªìn': ['Bu·ªìn nh∆∞ cha ch·∫øt', 'Bu·ªìn nh∆∞ tr·∫•u c·∫Øn'],
                'ti·ªÅn': ['Ti·ªÅn l√† ti√™n l√† ph·∫≠t', 'C√≥ ti·ªÅn mua ti√™n c≈©ng ƒë∆∞·ª£c'],
            }
            idioms = common_idioms.get(word.lower(), [])
            for idiom in idioms:
                usages.append(f"üó£Ô∏è {idiom}")
        
        return usages[:5]
    
    def scrape_vietnamese_etymology(self, word):
        """Scrape t·ª´ nguy√™n ti·∫øng Vi·ªát"""
        etymologies = []
        
        try:
            # T√¨m t·ª´ nguy√™n tr√™n wiktionary
            url = f"https://vi.wiktionary.org/wiki/{word}"
            response = requests.get(url, headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # T√¨m ph·∫ßn t·ª´ nguy√™n
                etymology_section = soup.find('span', {'id': 'T·ª´_nguy√™n'})
                if etymology_section:
                    parent = etymology_section.find_parent('h3')
                    if parent:
                        next_elem = parent.find_next_sibling(['p', 'div'])
                        if next_elem:
                            text = next_elem.get_text(strip=True)[:300]
                            etymologies.append(f"üìñ {text}")
        
        except:
            pass
        
        # Th√™m t·ª´ nguy√™n m·∫´u n·∫øu scrape kh√¥ng c√≥ k·∫øt qu·∫£
        if not etymologies:
            common_etymologies = {
                'ƒë·∫πp': 'T·ª´ H√°n Vi·ªát "ƒë·∫πp" c√≥ g·ªëc t·ª´ ch·ªØ H√°n Âæó (ƒë·∫Øc) - ƒë∆∞·ª£c, ƒë·∫°t ƒë∆∞·ª£c',
                't·ªët': 'T·ª´ thu·∫ßn Vi·ªát, c√≥ nghƒ©a g·ªëc l√† ch·∫•t l∆∞·ª£ng cao, ho√†n h·∫£o',
                'nhanh': 'T·ª´ thu·∫ßn Vi·ªát, ch·ªâ t·ªëc ƒë·ªô cao, mau l·∫π',
                'th√¥ng minh': 'T·ª´ H√°n Vi·ªát: th√¥ng (ÈÄö) - th√¥ng su·ªët, minh (Êòé) - s√°ng su·ªët',
                'h·ªçc': 'T·ª´ H√°n Vi·ªát: h·ªçc (Â≠∏) - h·ªçc t·∫≠p, nghi√™n c·ª©u',
                'tr∆∞·ªùng': 'T·ª´ H√°n Vi·ªát: tr∆∞·ªùng (Â†¥) - n∆°i, ƒë·ªãa ƒëi·ªÉm ho·∫°t ƒë·ªông',
                'nh√†': 'T·ª´ thu·∫ßn Vi·ªát, ch·ªâ n∆°i ·ªü, gia ƒë√¨nh',
                'n∆∞·ªõc': 'T·ª´ thu·∫ßn Vi·ªát, ch·ªâ ch·∫•t l·ªèng ho·∫∑c qu·ªëc gia',
                'm·∫π': 'T·ª´ thu·∫ßn Vi·ªát, g·ªëc M√¥n-Khmer, ch·ªâ ng∆∞·ªùi sinh th√†nh',
                'cha': 'T·ª´ thu·∫ßn Vi·ªát, g·ªëc M√¥n-Khmer, ch·ªâ ng∆∞·ªùi sinh th√†nh',
            }
            etymology = common_etymologies.get(word.lower())
            if etymology:
                etymologies.append(f"üìñ {etymology}")
        
        return etymologies[:3]
    
    def scrape_comprehensive_vietnamese_data(self, word):
        """Scrape to√†n b·ªô d·ªØ li·ªáu ti·∫øng Vi·ªát"""
        with st.spinner(f"üîç ƒêang thu th·∫≠p d·ªØ li·ªáu cho t·ª´ '{word}' t·ª´ web..."):
            data = {
                'definitions': self.scrape_vietnamese_definition(word),
                'examples': self.scrape_vietnamese_examples(word),
                'synonyms': self.scrape_vietnamese_synonyms(word),
                'antonyms': self.scrape_vietnamese_antonyms(word),
                'usages': self.scrape_vietnamese_usage(word),
                'etymologies': self.scrape_vietnamese_etymology(word),
                'scraped': True
            }
        
        # ƒê√°nh d·∫•u ngu·ªìn d·ªØ li·ªáu
        data['sources'] = [
            "üåê Google Search",
            "üìò Vtudien.com", 
            "üìô Wiktionary",
            "üîç Web Scraping"
        ]
        
        return data

# ==================== L·ªöP X·ª¨ L√ù GI·ªåNG N√ìI ====================

class VoiceSearchSimple:
    """L·ªõp x·ª≠ l√Ω t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i ƒë∆°n gi·∫£n"""
    
    def __init__(self):
        self.recognizer = sr.Recognizer()
        
    def recognize_audio_file(self, audio_file, language="vi-VN"):
        """Nh·∫≠n di·ªán gi·ªçng n√≥i t·ª´ file audio upload"""
        try:
            # L∆∞u file t·∫°m
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                tmp_file.write(audio_file.read())
                tmp_path = tmp_file.name
            
            # Nh·∫≠n di·ªán t·ª´ file
            with sr.AudioFile(tmp_path) as source:
                audio_data = self.recognizer.record(source)
                text = self.recognizer.recognize_google(audio_data, language=language)
                
            # X√≥a file t·∫°m
            os.unlink(tmp_path)
            return text.lower() if text else None
            
        except sr.UnknownValueError:
            return None
        except sr.RequestError:
            st.error("üåê L·ªói k·∫øt n·ªëi d·ªãch v·ª• nh·∫≠n di·ªán gi·ªçng n√≥i.")
            return None
        except Exception as e:
            st.error(f"‚ùå L·ªói x·ª≠ l√Ω audio: {str(e)}")
            return None
    
    def process_voice_command(self, command_text):
        """X·ª≠ l√Ω c√¢u l·ªánh b·∫±ng gi·ªçng n√≥i"""
        if not command_text:
            return None
        
        # Lo·∫°i b·ªè t·ª´ d∆∞ th·ª´a
        keywords = ["t√¨m t·ª´", "tra t·ª´", "t·ª´ ƒëi·ªÉn", "d·ªãch", "translate", "search", "t√¨m ki·∫øm"]
        for keyword in keywords:
            if keyword in command_text:
                command_text = command_text.replace(keyword, "").strip()
        
        return command_text.strip()

# ==================== L·ªöP API DICTIONARY N√ÇNG C·∫§P ====================

class EnhancedDictionaryAPI:
    """L·ªõp qu·∫£n l√Ω c√°c ngu·ªìn API v√† web scraping"""
    
    def __init__(self):
        self.used_sources = set()
        self.web_scraper = WebScraper()
    
    def get_free_dictionary_api(self, word):
        """Free Dictionary API - Ngu·ªìn ch√≠nh ·ªïn ƒë·ªãnh"""
        try:
            url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                self.used_sources.add("Free Dictionary API")
                return response.json()
        except:
            pass
        return None
    
    def get_wordnet_enhanced(self, word):
        """WordNet v·ªõi m·ªü r·ªông h·ªçc thu·∫≠t"""
        try:
            synsets = wordnet.synsets(word)
            if not synsets:
                return None
            
            result = {
                'definitions': [],
                'synonyms': set(),
                'antonyms': set(),
                'examples': [],
                'semantic_relations': [],
                'word_family': [],
                'pos': []
            }
            
            for syn in synsets[:3]:
                # ƒê·ªãnh nghƒ©a
                result['definitions'].append({
                    'definition': syn.definition(),
                    'pos': syn.pos(),
                    'lexname': syn.lexname()
                })
                
                # T·ª´ lo·∫°i
                pos_map = {'n': 'Danh t·ª´', 'v': 'ƒê·ªông t·ª´', 'a': 'T√≠nh t·ª´', 'r': 'Tr·∫°ng t·ª´'}
                pos = pos_map.get(syn.pos(), 'Kh√¥ng x√°c ƒë·ªãnh')
                if pos not in result['pos']:
                    result['pos'].append(pos)
                
                # Quan h·ªá ng·ªØ nghƒ©a
                for hypernym in syn.hypernyms()[:2]:
                    result['semantic_relations'].append(f"T·ªïng qu√°t: {hypernym.name()}")
                
                # T·ª´ ƒë·ªìng nghƒ©a
                for lemma in syn.lemmas():
                    if lemma.name().lower() != word.lower():
                        result['synonyms'].add(lemma.name())
                    # T·ª´ tr√°i nghƒ©a
                    if lemma.antonyms():
                        result['antonyms'].add(lemma.antonyms()[0].name())
                
                # V√≠ d·ª•
                if syn.examples():
                    result['examples'].extend(syn.examples()[:2])
            
            result['synonyms'] = list(result['synonyms'])[:10]
            result['antonyms'] = list(result['antonyms'])[:10]
            
            self.used_sources.add("WordNet Database")
            return result
        except:
            return None
    
    def get_academic_data(self, word):
        """D·ªØ li·ªáu t·ª´ Academic Word List"""
        word_lower = word.lower()
        if word_lower in ACADEMIC_WORD_LIST_FULL:
            self.used_sources.add("Academic Word List")
            return {
                'academic_info': ACADEMIC_WORD_LIST_FULL[word_lower],
                'is_academic': True
            }
        return None
    
    def get_collocations_data(self, word):
        """Collocations t·ª´ database"""
        collocations = [
            'personal computer', 'computer system', 'computer program', 'computer science',
            'computer network', 'computer screen', 'computer virus', 'computer hardware',
            'computer software', 'computer literacy', 'computer engineering', 'computer lab',
            'learn quickly', 'learn English', 'learn something new', 'learn from mistakes',
            'learn by heart', 'learn by doing', 'learn the ropes', 'learn a lesson',
            'study hard', 'study English', 'study abroad', 'study materials',
            'study group', 'study session', 'case study', 'feasibility study',
            'research paper', 'scientific research', 'conduct research', 'research method',
            'make decision', 'make progress', 'make effort', 'make mistake',
            'take exam', 'take notes', 'take action', 'take responsibility',
            'have experience', 'have opportunity', 'have difficulty', 'have impact'
        ]
        
        # L·ªçc collocations c√≥ ch·ª©a t·ª´
        filtered = [c for c in collocations if word.lower() in c.lower()]
        if filtered:
            self.used_sources.add("Collocation Database")
            return {'collocations': filtered[:15]}
        return None
    
    def get_vietnamese_data_from_web(self, word):
        """L·∫•y d·ªØ li·ªáu ti·∫øng Vi·ªát t·ª´ web scraping"""
        try:
            data = self.web_scraper.scrape_comprehensive_vietnamese_data(word)
            self.used_sources.add("Web Scraping")
            self.used_sources.update(data.get('sources', []))
            return data
        except Exception as e:
            st.warning(f"Web scraping kh√¥ng th√†nh c√¥ng: {str(e)}")
            return None
    
    def get_context_examples(self, word):
        """V√≠ d·ª• ng·ªØ c·∫£nh"""
        examples = [
            f"I use my {word} for work and entertainment every day.",
            f"The {word} processes data at incredible speed.",
            f"She is studying {word} science at university.",
            f"Modern {word}s can perform billions of operations per second.",
            f"{word.title()} technology has revolutionized our lives.",
            f"Children learn {word}s more easily than adults.",
            f"We should learn from our {word}s to avoid repeating them.",
            f"It's important to learn how to {word} in today's digital world."
        ]
        
        # L·ªçc examples c√≥ ch·ª©a t·ª´
        filtered = [e for e in examples if word.lower() in e.lower()]
        if filtered:
            self.used_sources.add("Context Examples Database")
            return {'examples': filtered[:6]}
        return None
    
    def get_word_frequency(self, word):
        """Th√¥ng tin t·∫ßn su·∫•t s·ª≠ d·ª•ng"""
        frequency_data = {
            'computer': {'level': 'R·∫•t th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 95},
            'learn': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 98},
            'study': {'level': 'R·∫•t th√¥ng d·ª•ng', 'frequency': 'A2', 'score': 90},
            'research': {'level': 'Th√¥ng d·ª•ng', 'frequency': 'B1', 'score': 85},
            'develop': {'level': 'Th√¥ng d·ª•ng', 'frequency': 'B1', 'score': 80},
            'important': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 96},
            'time': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 99},
            'people': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 97},
            'year': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 96},
            'work': {'level': 'C·ª±c k·ª≥ th√¥ng d·ª•ng', 'frequency': 'A1', 'score': 95},
        }
        
        data = frequency_data.get(
            word.lower(), 
            {'level': 'Th√¥ng d·ª•ng', 'frequency': 'B1', 'score': 75}
        )
        self.used_sources.add("Word Frequency Database")
        return data
    
    def get_collocation_patterns(self, word):
        """M·∫´u collocation t·ª´ ph√¢n t√≠ch t·ª´ lo·∫°i"""
        try:
            synsets = wordnet.synsets(word)
            if not synsets:
                return []
            
            pos = synsets[0].pos()
            pos_map = {'v': 'verb', 'n': 'noun', 'a': 'adjective', 'r': 'adverb'}
            word_pos = pos_map.get(pos, 'noun')
            
            patterns = {
                'verb': ['verb + noun', 'verb + adverb', 'verb + preposition', 'phrasal verb'],
                'noun': ['adjective + noun', 'noun + verb', 'noun + of + noun', 'compound noun'],
                'adjective': ['adverb + adjective', 'adjective + noun', 'adjective + preposition'],
                'adverb': ['verb + adverb', 'adverb + adjective']
            }
            
            return patterns.get(word_pos, [])
        except:
            return []
    
    def get_semantic_nuance(self, word):
        """Ph√¢n t√≠ch s·∫Øc th√°i √Ω nghƒ©a"""
        try:
            synsets = wordnet.synsets(word)
            if not synsets:
                return None
            
            main_synset = synsets[0]
            synonyms = list(set([lemma.name() for lemma in main_synset.lemmas() if lemma.name() != word]))
            
            nuance_analysis = {
                'word': word,
                'main_definition': main_synset.definition(),
                'pos': main_synset.pos(),
                'synonyms_comparison': [],
                'usage_level': self.get_word_frequency(word)['level']
            }
            
            for synonym in synonyms[:4]:
                syn_synsets = wordnet.synsets(synonym)
                if syn_synsets:
                    syn_def = syn_synsets[0].definition()
                    pos_map = {'n': 'danh t·ª´', 'v': 'ƒë·ªông t·ª´', 'a': 't√≠nh t·ª´', 'r': 'tr·∫°ng t·ª´'}
                    syn_pos = pos_map.get(syn_synsets[0].pos(), 'kh√¥ng x√°c ƒë·ªãnh')
                    
                    nuance_analysis['synonyms_comparison'].append({
                        'synonym': synonym,
                        'definition': syn_def,
                        'pos': syn_pos,
                        'difference': f"'{synonym}' ({syn_pos}): {syn_def}"
                    })
            
            return nuance_analysis
        except:
            return None

# ==================== GIAO DI·ªÜN NH·∫¨P GI·ªåNG N√ìI ====================

def voice_search_interface(input_key, language="vi-VN"):
    """Giao di·ªán t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i"""
    
    st.markdown("---")
    st.markdown("### üé§ T√åM KI·∫æM B·∫∞NG GI·ªåNG N√ìI")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("#### üìÅ Upload File Audio")
        
        # Upload file audio
        audio_file = st.file_uploader(
            "Ch·ªçn file √¢m thanh",
            type=["wav", "mp3", "m4a", "ogg"],
            key=f"audio_upload_{input_key}",
            help="Ghi √¢m t·ª´ c·∫ßn tra v√† upload file WAV/MP3"
        )
        
        if audio_file is not None:
            # Hi·ªÉn th·ªã th√¥ng tin file
            st.write(f"üìÅ File: {audio_file.name}")
            st.write(f"üìä Size: {audio_file.size / 1024:.1f} KB")
            
            # N√∫t x·ª≠ l√Ω
            if st.button("üéØ Nh·∫≠n di·ªán gi·ªçng n√≥i", key=f"process_{input_key}"):
                with st.spinner("ƒêang x·ª≠ l√Ω gi·ªçng n√≥i..."):
                    try:
                        # Nh·∫≠n di·ªán
                        recognized_text = voice_search.recognize_audio_file(audio_file, language=language)
                        
                        if recognized_text:
                            # X·ª≠ l√Ω c√¢u l·ªánh
                            processed_text = voice_search.process_voice_command(recognized_text)
                            
                            if processed_text:
                                st.success(f"‚úÖ ƒê√£ nh·∫≠n di·ªán: **{processed_text}**")
                                # L∆∞u v√†o session state v√† t·ª± ƒë·ªông search
                                st.session_state[input_key] = processed_text
                                st.rerun()
                            else:
                                st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n ƒë√£ nh·∫≠n di·ªán")
                        else:
                            st.error("Kh√¥ng th·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i. H√£y th·ª≠ l·∫°i v·ªõi file ch·∫•t l∆∞·ª£ng t·ªët h∆°n.")
                            
                    except Exception as e:
                        st.error(f"L·ªói: {str(e)}")
    
    with col2:
        st.markdown("#### üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
        st.info("""
        **C√°ch s·ª≠ d·ª•ng t√≠nh nƒÉng gi·ªçng n√≥i:**
        
        1. **Ghi √¢m t·ª´ c·∫ßn tra:**
           - D√πng ƒëi·ªán tho·∫°i ho·∫∑c m√°y t√≠nh ghi √¢m
           - N√≥i r√µ r√†ng t·ª´ ti·∫øng Anh ho·∫∑c ti·∫øng Vi·ªát
           - L∆∞u file d∆∞·ªõi d·∫°ng WAV, MP3
        
        2. **Upload file:**
           - Ch·ªçn file √¢m thanh ƒë√£ ghi
           - Nh·∫•n n√∫t "Nh·∫≠n di·ªán gi·ªçng n√≥i"
           - H·ªá th·ªëng t·ª± ƒë·ªông nh·∫≠n di·ªán v√† tra t·ª´
        
        3. **M·∫πo ƒë·ªÉ nh·∫≠n di·ªán ch√≠nh x√°c:**
           - N√≥i trong m√¥i tr∆∞·ªùng y√™n tƒ©nh
           - N√≥i r√µ r√†ng, ch·∫≠m r√£i
           - Microphone g·∫ßn mi·ªáng
           - File √¢m thanh ch·∫•t l∆∞·ª£ng t·ªët
        
        **H·ªó tr·ª£ ng√¥n ng·ªØ:**
        - Ti·∫øng Anh (English)
        - Ti·∫øng Vi·ªát (Vietnamese)
        """)

# ==================== C√ÅC H√ÄM HI·ªÇN TH·ªä CH√çNH ====================

def display_web_scraping_results(word, web_data):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ web scraping"""
    if not web_data:
        return
    
    st.markdown("### üåê D·ªÆ LI·ªÜU T·ª™ WEB SCRAPING")
    
    # Hi·ªÉn th·ªã ngu·ªìn
    if web_data.get('sources'):
        st.write("**Ngu·ªìn d·ªØ li·ªáu web:**")
        cols = st.columns(4)
        for i, source in enumerate(web_data['sources'][:4]):
            with cols[i % 4]:
                st.markdown(f'<span class="scraping-badge">{source}</span>', unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã ƒë·ªãnh nghƒ©a
    if web_data.get('definitions'):
        st.markdown("#### üìù ƒê·ªãnh nghƒ©a t·ª´ web")
        for definition in web_data['definitions']:
            st.markdown(f"""
            <div class="web-data-card">
                {definition}
            </div>
            """, unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã v√≠ d·ª•
    if web_data.get('examples'):
        st.markdown("#### üí¨ V√≠ d·ª• t·ª´ web")
        for example in web_data['examples'][:3]:
            st.markdown(f"""
            <div class="context-card">
                {example}
            </div>
            """, unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã t·ª´ ƒë·ªìng nghƒ©a v√† tr√°i nghƒ©a
    col1, col2 = st.columns(2)
    
    with col1:
        if web_data.get('synonyms'):
            st.markdown("#### üîÑ T·ª´ ƒë·ªìng nghƒ©a")
            for synonym in web_data['synonyms'][:8]:
                st.markdown(f'''
                <div class="synonym-card">
                    <div style="font-weight: bold;">{synonym}</div>
                </div>
                ''', unsafe_allow_html=True)
    
    with col2:
        if web_data.get('antonyms'):
            st.markdown("#### ‚ö° T·ª´ tr√°i nghƒ©a")
            for antonym in web_data['antonyms'][:8]:
                st.markdown(f'''
                <div class="antonym-card">
                    <div style="font-weight: bold;">{antonym}</div>
                </div>
                ''', unsafe_allow_html=True)
    
    # Hi·ªÉn th·ªã th√†nh ng·ªØ
    if web_data.get('usages'):
        st.markdown("#### üó£Ô∏è Th√†nh ng·ªØ, t·ª•c ng·ªØ")
        for usage in web_data['usages'][:3]:
            st.info(usage)
    
    # Hi·ªÉn th·ªã t·ª´ nguy√™n
    if web_data.get('etymologies'):
        st.markdown("#### üìñ T·ª´ nguy√™n")
        for etymology in web_data['etymologies']:
            st.success(etymology)

# (C√°c h√†m display_academic_words_section, display_english_vietnamese_advanced, 
# display_vietnamese_english, display_vietnamese_vietnamese gi·ªØ nguy√™n nh∆∞ code tr∆∞·ªõc,
# nh∆∞ng s·ª≠ d·ª•ng EnhancedDictionaryAPI thay v√¨ StableDictionaryAPI)

# ==================== PH·∫¶N ANH-VI·ªÜT N√ÇNG CAO ====================

def display_english_vietnamese_advanced():
    """Hi·ªÉn th·ªã ph·∫ßn Anh-Vi·ªát"""
    st.markdown('<div class="sub-header" id="english-vietnamese">üîç TRA T·ª™ ANH - VI·ªÜT</div>', unsafe_allow_html=True)
    
    # Ki·ªÉm tra n·∫øu c√≥ t·ª´ ƒë∆∞·ª£c ch·ªçn t·ª´ danh s√°ch h·ªçc thu·∫≠t
    if 'selected_academic_word' in st.session_state and st.session_state.selected_academic_word:
        default_word = st.session_state.selected_academic_word
        del st.session_state.selected_academic_word
    else:
        default_word = st.session_state.get("advanced_en_input", "")
    
    # Giao di·ªán t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i
    voice_search_interface("advanced_en_input", language="en-US")
    
    # √î nh·∫≠p li·ªáu
    col1, col2 = st.columns([3, 1])
    with col1:
        en_word = st.text_input(
            "Nh·∫≠p t·ª´ ti·∫øng Anh:",
            placeholder="computer, analyze, research...",
            key="advanced_en_input",
            value=default_word
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        search_clicked = st.button("üöÄ **TRA T·ª™**", key="advanced_search", use_container_width=True)
    
    if (search_clicked and en_word) or (en_word and en_word != st.session_state.get("last_searched", "")):
        st.session_state["last_searched"] = en_word
        
        with st.spinner("ƒêang ph√¢n t√≠ch t·ª´ v·ª±ng v·ªõi ƒëa ngu·ªìn d·ªØ li·ªáu..."):
            try:
                api_handler = EnhancedDictionaryAPI()
                
                # D·ªãch sang ti·∫øng Vi·ªát
                trans = translator.translate(en_word, src='en', dest='vi')
                
                # L·∫•y IPA
                try:
                    ipa_text = ipa.convert(en_word)
                except:
                    ipa_text = "[Kh√¥ng t√¨m th·∫•y phi√™n √¢m]"
                
                # G·ªçi c√°c API v√† database
                free_dict_data = api_handler.get_free_dictionary_api(en_word)
                wordnet_data = api_handler.get_wordnet_enhanced(en_word)
                academic_data = api_handler.get_academic_data(en_word)
                collocations_data = api_handler.get_collocations_data(en_word)
                context_data = api_handler.get_context_examples(en_word)
                frequency_data = api_handler.get_word_frequency(en_word)
                nuance_data = api_handler.get_semantic_nuance(en_word)
                
                # Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n
                st.markdown(f'''
                <div class="word-card">
                    <h2 style="margin:0; color:#0d47a1; font-size: 2.5rem;">{en_word.title()}</h2>
                    <div class="ipa-text">/{ipa_text}/</div>
                    <h3 style="color:#1565c0; margin-top:1rem; font-size: 1.5rem;">üìñ {trans.text}</h3>
                </div>
                ''', unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã c√°c ngu·ªìn ƒë√£ s·ª≠ d·ª•ng
                if api_handler.used_sources:
                    st.subheader("üìö NGU·ªíN D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C S·ª¨ D·ª§NG")
                    cols = st.columns(4)
                    sources = list(api_handler.used_sources)
                    for i, source in enumerate(sources):
                        with cols[i % 4]:
                            if "Scraping" in source or "Google" in source or "Web" in source:
                                st.markdown(f'<span class="scraping-badge">{source}</span>', unsafe_allow_html=True)
                            else:
                                st.markdown(f'<span class="source-badge">{source}</span>', unsafe_allow_html=True)
                
                # Tabs th√¥ng tin chi ti·∫øt
                tab_names = ["üìù ƒê·ªãnh nghƒ©a", "ü§ù Collocation", "üé≠ Ng·ªØ c·∫£nh", "üé® S·∫Øc th√°i", "üéØ H·ªçc thu·∫≠t", "üìä Ph√¢n t√≠ch"]
                tabs = st.tabs(tab_names)
                
                with tabs[0]:  # ƒê·ªãnh nghƒ©a
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    if free_dict_data:
                        st.subheader("Free Dictionary API")
                        try:
                            meanings = free_dict_data[0].get('meanings', [])
                            for meaning in meanings[:3]:
                                part_of_speech = meaning.get('partOfSpeech', '')
                                definitions = meaning.get('definitions', [])
                                if definitions:
                                    st.write(f"**{part_of_speech}** - {definitions[0].get('definition', '')}")
                        except:
                            st.info("Kh√¥ng c√≥ ƒë·ªãnh nghƒ©a t·ª´ Free Dictionary API")
                    
                    if wordnet_data and wordnet_data.get('definitions'):
                        st.subheader("WordNet Database")
                        for i, definition in enumerate(wordnet_data['definitions'][:3]):
                            st.write(f"{i+1}. {definition.get('definition', '')}")
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with tabs[1]:  # Collocation
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    if collocations_data and collocations_data.get('collocations'):
                        st.markdown("### ü§ù COLLOCATION - T·ª´ th∆∞·ªùng ƒëi c√πng")
                        cols = st.columns(2)
                        for i, collocation in enumerate(collocations_data['collocations']):
                            with cols[i % 2]:
                                try:
                                    vi_trans = translator.translate(collocation, src='en', dest='vi').text
                                    st.markdown(f"""
                                    <div class="collocation-card">
                                        <div style="font-size: 1.1rem; font-weight: bold;">{collocation}</div>
                                        <div style="font-style: italic; margin-top: 0.5rem; font-size: 0.9rem;">{vi_trans}</div>
                                    </div>
                                    """, unsafe_allow_html=True)
                                except:
                                    st.markdown(f"""
                                    <div class="collocation-card">
                                        <div style="font-size: 1.1rem; font-weight: bold;">{collocation}</div>
                                    </div>
                                    """, unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with tabs[2]:  # Ng·ªØ c·∫£nh
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    if context_data and context_data.get('examples'):
                        st.markdown("### üé≠ NG·ªÆ C·∫¢NH S·ª¨ D·ª§NG")
                        for i, example in enumerate(context_data['examples'][:6]):
                            try:
                                vi_trans = translator.translate(example, src='en', dest='vi').text
                                st.markdown(f"""
                                <div class="context-card">
                                    <div style="font-weight: bold;">üìù V√≠ d·ª• {i+1}:</div>
                                    <div style="margin: 0.5rem 0; font-size: 1.1rem;">{example}</div>
                                    <div style="font-style: italic; color: #1976d2;">{vi_trans}</div>
                                </div>
                                """, unsafe_allow_html=True)
                            except:
                                st.markdown(f"""
                                <div class="context-card">
                                    <div style="font-weight: bold;">üìù V√≠ d·ª• {i+1}:</div>
                                    <div style="margin: 0.5rem 0; font-size: 1.1rem;">{example}</div>
                                </div>
                                """, unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with tabs[3]:  # S·∫Øc th√°i
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    if nuance_data:
                        st.markdown("### üé® S·∫ÆC TH√ÅI √ù NGHƒ®A")
                        st.write(f"**ƒê·ªãnh nghƒ©a ch√≠nh c·ªßa '{en_word}':**")
                        st.info(nuance_data['main_definition'])
                        
                        if nuance_data['synonyms_comparison']:
                            st.markdown("#### üìä Ph√¢n t√≠ch so s√°nh v·ªõi t·ª´ ƒë·ªìng nghƒ©a")
                            for comparison in nuance_data['synonyms_comparison']:
                                st.markdown(f"""
                                <div class="context-card">
                                    <div style="font-weight: bold; font-size: 1.1rem; color: #7b1fa2;">{comparison['synonym']}</div>
                                    <div style="margin: 0.5rem 0; font-size: 0.9rem; color: #6a1b9a;">({comparison['pos']})</div>
                                    <div style="margin: 0.5rem 0;">{comparison['definition']}</div>
                                </div>
                                """, unsafe_allow_html=True)
                    else:
                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu ph√¢n t√≠ch s·∫Øc th√°i cho t·ª´ n√†y")
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with tabs[4]:  # H·ªçc thu·∫≠t
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    if academic_data and academic_data.get('is_academic'):
                        info = academic_data['academic_info']
                        st.success("‚úÖ **T·ª´ v·ª±ng h·ªçc thu·∫≠t quan tr·ªçng**")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("C·∫•p ƒë·ªô", info['level'])
                        with col2:
                            st.metric("T·∫ßn su·∫•t", info['frequency'])
                        with col3:
                            st.metric("Ch·ªß ƒë·ªÅ", info['topic'])
                        
                        st.write(f"**Nghƒ©a ti·∫øng Vi·ªát:** {info['meaning']}")
                    else:
                        st.info("T·ª´ n√†y kh√¥ng n·∫±m trong danh s√°ch 120 t·ª´ v·ª±ng h·ªçc thu·∫≠t c·ªët l√µi")
                    st.markdown('</div>', unsafe_allow_html=True)
                
                with tabs[5]:  # Ph√¢n t√≠ch
                    st.markdown('<div class="tab-content">', unsafe_allow_html=True)
                    st.markdown("### üìä PH√ÇN T√çCH CHI TI·∫æT")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("M·ª©c ƒë·ªô ph·ªï bi·∫øn", frequency_data['level'])
                    with col2:
                        st.metric("C·∫•p ƒë·ªô CEFR", frequency_data['frequency'])
                    with col3:
                        st.metric("ƒêi·ªÉm ph·ªï bi·∫øn", f"{frequency_data['score']}/100")
                    
                    if wordnet_data:
                        if wordnet_data.get('synonyms'):
                            st.markdown("#### üîÑ T·ª´ ƒë·ªìng nghƒ©a")
                            synonyms_list = list(wordnet_data['synonyms'])[:8]
                            cols = st.columns(4)
                            for i, synonym in enumerate(synonyms_list):
                                with cols[i % 4]:
                                    st.markdown(f'''
                                    <div class="synonym-card">
                                        <div style="font-weight: bold;">{synonym}</div>
                                    </div>
                                    ''', unsafe_allow_html=True)
                        
                        if wordnet_data.get('examples'):
                            st.markdown("#### üìö V√≠ d·ª• s·ª≠ d·ª•ng")
                            for i, example in enumerate(wordnet_data['examples'][:3]):
                                try:
                                    vi_trans = translator.translate(example, src='en', dest='vi').text
                                    st.markdown(f'''
                                    <div class="example-card">
                                        <strong>V√≠ d·ª• {i+1}:</strong><br>
                                        {example}<br>
                                        <em>{vi_trans}</em>
                                    </div>
                                    ''', unsafe_allow_html=True)
                                except:
                                    st.markdown(f'''
                                    <div class="example-card">
                                        <strong>V√≠ d·ª• {i+1}:</strong><br>
                                        {example}
                                    </div>
                                    ''', unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω t·ª´ '{en_word}': {str(e)}")

# ==================== PH·∫¶N VI·ªÜT-ANH V·ªöI WEB SCRAPING ====================

def display_vietnamese_english():
    st.markdown('<div class="sub-header">üîç TRA T·ª™ VI·ªÜT - ANH</div>', unsafe_allow_html=True)
    
    # Giao di·ªán t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i
    voice_search_interface("vi_en_input", language="vi-VN")
    
    # √î nh·∫≠p li·ªáu
    col1, col2 = st.columns([3, 1])
    with col1:
        vi_word = st.text_input(
            "Nh·∫≠p t·ª´ ti·∫øng Vi·ªát:",
            placeholder="ph√¢n t√≠ch, nghi√™n c·ª©u, m√¥i tr∆∞·ªùng, ƒë·∫πp, t·ªët...",
            key="vi_en_input",
            value=st.session_state.get("vi_en_input", "")
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        search_clicked = st.button("üöÄ **PH√ÇN T√çCH V√Ä D·ªäCH**", key="vi_en_search", use_container_width=True)
    
    if search_clicked and vi_word:
        with st.spinner("ƒêang thu th·∫≠p d·ªØ li·ªáu t·ª´ web v√† d·ªãch..."):
            try:
                api_handler = EnhancedDictionaryAPI()
                
                # D·ªãch sang ti·∫øng Anh
                trans = translator.translate(vi_word, src='vi', dest='en')
                en_word = trans.text
                
                # L·∫•y d·ªØ li·ªáu t·ª´ web scraping (ti·∫øng Vi·ªát)
                web_data = api_handler.get_vietnamese_data_from_web(vi_word)
                
                # L·∫•y d·ªØ li·ªáu cho t·ª´ ti·∫øng Anh
                try:
                    ipa_text = ipa.convert(en_word)
                except:
                    ipa_text = "[Kh√¥ng t√¨m th·∫•y phi√™n √¢m]"
                
                wordnet_data = api_handler.get_wordnet_enhanced(en_word)
                academic_data = api_handler.get_academic_data(en_word)
                frequency_data = api_handler.get_word_frequency(en_word)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.markdown(f'''
                <div class="vietnamese-card">
                    <h2 style="margin:0; color:#1b5e20;">{vi_word.title()}</h2>
                    <h3 style="color:#2e7d32; margin:0.5rem 0;">‚Üí {en_word.title()} /{ipa_text}/</h3>
                </div>
                ''', unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã ngu·ªìn d·ªØ li·ªáu
                if api_handler.used_sources:
                    st.write("**üìö Ngu·ªìn d·ªØ li·ªáu:**")
                    cols = st.columns(4)
                    sources = list(api_handler.used_sources)
                    for i, source in enumerate(sources):
                        with cols[i % 4]:
                            if "Scraping" in source or "Google" in source or "Web" in source:
                                st.markdown(f'<span class="scraping-badge">{source}</span>', unsafe_allow_html=True)
                            else:
                                st.markdown(f'<span class="source-badge">{source}</span>', unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ web scraping
                if web_data:
                    display_web_scraping_results(vi_word, web_data)
                
                # Th√¥ng tin t·ª´ ti·∫øng Anh
                st.markdown("---")
                st.markdown("### üá∫üá∏ TH√îNG TIN T·ª™ TI·∫æNG ANH")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if wordnet_data and wordnet_data.get('synonyms'):
                        st.markdown("#### üìó T·ª´ ƒë·ªìng nghƒ©a (ti·∫øng Anh)")
                        synonyms_list = list(wordnet_data['synonyms'])[:6]
                        for synonym in synonyms_list:
                            st.write(f"- {synonym}")
                
                with col2:
                    if academic_data:
                        st.success("‚úÖ **T·ª´ v·ª±ng h·ªçc thu·∫≠t**")
                        info = academic_data['academic_info']
                        st.write(f"**C·∫•p ƒë·ªô:** {info['level']}")
                        st.write(f"**Ch·ªß ƒë·ªÅ:** {info['topic']}")
                        st.write(f"**Nghƒ©a:** {info['meaning']}")
                
                # Th√¥ng tin t·∫ßn su·∫•t
                st.markdown("#### üìä Th√¥ng tin t·∫ßn su·∫•t")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("M·ª©c ƒë·ªô ph·ªï bi·∫øn", frequency_data['level'])
                with col2:
                    st.metric("C·∫•p ƒë·ªô CEFR", frequency_data['frequency'])
                with col3:
                    st.metric("ƒêi·ªÉm", f"{frequency_data['score']}/100")
                
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω t·ª´ '{vi_word}': {str(e)}")

# ==================== PH·∫¶N VI·ªÜT-VI·ªÜT V·ªöI WEB SCRAPING ====================

def display_vietnamese_vietnamese():
    """Hi·ªÉn th·ªã ph·∫ßn Vi·ªát-Vi·ªát v·ªõi web scraping"""
    st.markdown('<div class="sub-header">üî§ T·ª™ ƒêI·ªÇN VI·ªÜT - VI·ªÜT (WEB SCRAPING)</div>', unsafe_allow_html=True)
    
    # Giao di·ªán t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i
    voice_search_interface("vi_vi_input", language="vi-VN")
    
    # √î nh·∫≠p li·ªáu
    col1, col2 = st.columns([3, 1])
    with col1:
        vi_word = st.text_input(
            "Nh·∫≠p t·ª´ ti·∫øng Vi·ªát:",
            placeholder="v√≠ d·ª•: ƒë·∫πp, t·ªët, nhanh, th√¥ng minh, h·∫°nh ph√∫c...",
            key="vi_vi_input",
            value=st.session_state.get("vi_vi_input", "")
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        search_clicked = st.button("üîç **PH√ÇN T√çCH**", key="vi_vi_search", use_container_width=True)
    
    if search_clicked and vi_word:
        with st.spinner("ƒêang thu th·∫≠p v√† ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ web..."):
            try:
                api_handler = EnhancedDictionaryAPI()
                
                # L·∫•y d·ªØ li·ªáu t·ª´ web scraping
                web_data = api_handler.get_vietnamese_data_from_web(vi_word)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ ch√≠nh
                st.markdown(f'''
                <div class="vietnamese-card">
                    <h2 style="margin:0; color:#1b5e20;">{vi_word.title()}</h2>
                    <h3 style="color:#2e7d32; margin-top:1rem;">üìö Ph√¢n t√≠ch t·ª´ ti·∫øng Vi·ªát t·ª´ web</h3>
                </div>
                ''', unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã ngu·ªìn d·ªØ li·ªáu
                if api_handler.used_sources:
                    st.write("**üåê Ngu·ªìn d·ªØ li·ªáu web:**")
                    cols = st.columns(4)
                    sources = list(api_handler.used_sources)
                    for i, source in enumerate(sources):
                        with cols[i % 4]:
                            st.markdown(f'<span class="scraping-badge">{source}</span>', unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ web scraping ƒë·∫ßy ƒë·ªß
                if web_data:
                    display_web_scraping_results(vi_word, web_data)
                else:
                    st.warning("Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu t·ª´ web. ƒêang s·ª≠ d·ª•ng database c·ªë ƒë·ªãnh...")
                    
                    # Hi·ªÉn th·ªã database c·ªë ƒë·ªãnh
                    common_words = {
                        'ƒë·∫πp': {
                            'synonyms': ['xinh', 'xinh ƒë·∫πp', 'tuy·ªát ƒë·∫πp', 'l·ªông l·∫´y', 'duy√™n d√°ng'],
                            'antonyms': ['x·∫•u', 'x·∫•u x√≠', 'kh√≥ coi', 'th√¥ k·ªách'],
                            'examples': ['C√¥ ·∫•y r·∫•t ƒë·∫πp.', 'C·∫£nh ƒë·∫πp l√†m say l√≤ng ng∆∞·ªùi.', 'B·ª©c tranh ƒë·∫πp qu√°!']
                        },
                        't·ªët': {
                            'synonyms': ['tuy·ªát v·ªùi', 'xu·∫•t s·∫Øc', 'ho√†n h·∫£o', '∆∞u t√∫'],
                            'antonyms': ['x·∫•u', 't·ªìi', 'k√©m', 't·ªá h·∫°i'],
                            'examples': ['Anh ·∫•y l√† ng∆∞·ªùi r·∫•t t·ªët.', 'Th·ªùi ti·∫øt h√¥m nay th·∫≠t t·ªët.', 'K·∫øt qu·∫£ h·ªçc t·∫≠p r·∫•t t·ªët.']
                        },
                        'nhanh': {
                            'synonyms': ['mau', 'nhanh ch√≥ng', 'th·∫ßn t·ªëc', 'ch√≥ng v√°nh'],
                            'antonyms': ['ch·∫≠m', 'ch·∫≠m ch·∫°p', '√¨ ·∫°ch', 'r·ªÅ r√†'],
                            'examples': ['Anh ta ch·∫°y r·∫•t nhanh.', 'C√¥ ·∫•y h·ªçc r·∫•t nhanh.', 'Xe n√†y ch·∫°y nhanh th·∫≠t.']
                        }
                    }
                    
                    if vi_word.lower() in common_words:
                        data = common_words[vi_word.lower()]
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("#### üîÑ T·ª´ ƒë·ªìng nghƒ©a")
                            for synonym in data['synonyms']:
                                st.write(f"- {synonym}")
                        with col2:
                            st.markdown("#### ‚ö° T·ª´ tr√°i nghƒ©a")
                            for antonym in data['antonyms']:
                                st.write(f"- {antonym}")
                        
                        st.markdown("#### üí¨ V√≠ d·ª•")
                        for example in data['examples']:
                            st.info(example)
                    else:
                        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu cho t·ª´ n√†y trong database c·ªë ƒë·ªãnh")
                
                # Ph·∫ßn ghi ch√∫ h·ªçc t·∫≠p
                st.markdown("---")
                st.markdown("### üìö GHI CH√ö H·ªåC T·∫¨P")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("""
                    **Ph∆∞∆°ng ph√°p h·ªçc t·ª´ v·ª±ng:**
                    1. Ghi ch√©p t·ª´ theo ch·ªß ƒë·ªÅ
                    2. H·ªçc t·ª´ qua ng·ªØ c·∫£nh
                    3. √în t·∫≠p ƒë·ªãnh k·ª≥
                    4. S·ª≠ d·ª•ng flashcard
                    5. ƒê·ªçc s√°ch b√°o ti·∫øng Vi·ªát
                    """)
                
                with col2:
                    st.markdown("""
                    **M·∫πo l√†m b√†i thi:**
                    - ƒê·ªçc k·ªπ ƒë·ªÅ b√†i
                    - Qu·∫£n l√Ω th·ªùi gian
                    - Ki·ªÉm tra ƒë√°p √°n
                    - Ch√∫ √Ω ng·ªØ c·∫£nh
                    - Kh√¥ng b·ªè tr·ªëng c√¢u
                    """)
                
            except Exception as e:
                st.error(f"L·ªói khi ph√¢n t√≠ch t·ª´ '{vi_word}': {str(e)}")

# ==================== PH·∫¶N T·ª™ V·ª∞NG H·ªåC THU·∫¨T ====================

def display_academic_words_section():
    """Hi·ªÉn th·ªã danh s√°ch 120 t·ª´ v·ª±ng h·ªçc thu·∫≠t"""
    st.markdown('<div class="sub-header">üìö 120 T·ª™ V·ª∞NG H·ªåC THU·∫¨T C·ªêT L√ïI (AWL)</div>', unsafe_allow_html=True)
    
    # T√¨m ki·∫øm trong danh s√°ch
    search_term = st.text_input("üîç T√¨m t·ª´ v·ª±ng h·ªçc thu·∫≠t:", placeholder="Nh·∫≠p t·ª´ c·∫ßn t√¨m...")
    
    # Ph√¢n lo·∫°i theo c·∫•p ƒë·ªô
    level_filter = st.selectbox("L·ªçc theo c·∫•p ƒë·ªô:", ["T·∫•t c·∫£", "A1-A2", "B1", "B2"])
    
    # S·∫Øp x·∫øp
    sort_by = st.selectbox("S·∫Øp x·∫øp theo:", ["Th·ª© t·ª± A-Z", "C·∫•p ƒë·ªô", "Ch·ªß ƒë·ªÅ"])
    
    # L·ªçc t·ª´ v·ª±ng
    filtered_words = {}
    for word, info in ACADEMIC_WORD_LIST_FULL.items():
        if search_term and search_term.lower() not in word.lower():
            continue
        
        if level_filter == "A1-A2" and info['level'] not in ['A1', 'A2']:
            continue
        elif level_filter == "B1" and info['level'] != 'B1':
            continue
        elif level_filter == "B2" and info['level'] != 'B2':
            continue
        
        filtered_words[word] = info
    
    # S·∫Øp x·∫øp
    if sort_by == "Th·ª© t·ª± A-Z":
        sorted_words = sorted(filtered_words.items())
    elif sort_by == "C·∫•p ƒë·ªô":
        sorted_words = sorted(filtered_words.items(), key=lambda x: x[1]['level'])
    else:  # Ch·ªß ƒë·ªÅ
        sorted_words = sorted(filtered_words.items(), key=lambda x: x[1]['topic'])
    
    # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng
    st.write(f"**T√¨m th·∫•y {len(sorted_words)} t·ª´ v·ª±ng**")
    
    # Hi·ªÉn th·ªã t·ª´ v·ª±ng d·∫°ng grid
    cols_per_row = 5
    words_displayed = 0
    
    for i in range(0, len(sorted_words), cols_per_row):
        cols = st.columns(cols_per_row)
        for j in range(cols_per_row):
            if i + j < len(sorted_words):
                word, info = sorted_words[i + j]
                with cols[j]:
                    # T·∫°o th·∫ª t·ª´ v·ª±ng
                    st.markdown(f'''
                    <div class="academic-word-card" onclick="window.location.href='#english-vietnamese'">
                        <div style="font-weight: bold; font-size: 1.1rem;">{word}</div>
                        <div style="font-size: 0.8rem; margin-top: 0.3rem;">{info['meaning']}</div>
                        <div style="font-size: 0.7rem; color: #666; margin-top: 0.2rem;">
                            {info['level']} ‚Ä¢ {info['topic']}
                        </div>
                    </div>
                    ''', unsafe_allow_html=True)
                    
                    # JavaScript ƒë·ªÉ chuy·ªÉn h∆∞·ªõng khi click
                    js_code = f'''
                    <script>
                    document.addEventListener('DOMContentLoaded', function() {{
                        const cards = document.querySelectorAll('.academic-word-card');
                        if (cards[{words_displayed}]) {{
                            cards[{words_displayed}].addEventListener('click', function() {{
                                // L∆∞u t·ª´ v√†o session storage
                                sessionStorage.setItem('selected_academic_word', '{word}');
                                // Reload ƒë·ªÉ t·ª´ ƒë∆∞·ª£c load v√†o √¥ input
                                window.location.reload();
                            }});
                        }}
                    }});
                    </script>
                    '''
                    st.components.v1.html(js_code, height=0)
                    words_displayed += 1
    
    # Th√¥ng tin v·ªÅ AWL
    with st.expander("üìñ Th√¥ng tin v·ªÅ Academic Word List"):
        st.markdown("""
        **Academic Word List (AWL) - Danh s√°ch t·ª´ v·ª±ng h·ªçc thu·∫≠t:**
        
        - **570 t·ª´ v·ª±ng h·ªçc thu·∫≠t quan tr·ªçng nh·∫•t** (phi√™n b·∫£n r√∫t g·ªçn: 240 t·ª´)
        - Ph·ªß 10% vƒÉn b·∫£n h·ªçc thu·∫≠t ti·∫øng Anh
        - Thi·∫øt y·∫øu cho c√°c k·ª≥ thi: IELTS, TOEFL, SAT, ƒêGNL
        
        **C·∫•p ƒë·ªô CEFR:**
        - **A1-A2**: S∆° c·∫•p (Basic)
        - **B1**: Trung c·∫•p (Intermediate)
        - **B2**: Trung cao c·∫•p (Upper Intermediate)
        
        **Ch·ªß ƒë·ªÅ ch√≠nh:**
        - Nghi√™n c·ª©u & Ph∆∞∆°ng ph√°p lu·∫≠n
        - Khoa h·ªçc & To√°n h·ªçc
        - Kinh t·∫ø & T√†i ch√≠nh
        - Lu·∫≠t & Ch√≠nh s√°ch
        - X√£ h·ªôi & VƒÉn h√≥a
        """)

# ==================== MAIN FUNCTION ====================

def main():
    st.markdown('<div class="main-container">', unsafe_allow_html=True)
    st.markdown('<div class="main-header">üìö V·ªû GHI ƒêI·ªÜN T·ª¨ H·ªñ TR·ª¢ H·ªåC T·ª™ V·ª∞NG SONG NG·ªÆ ANH - VI·ªÜT</div>', unsafe_allow_html=True)
    
    # Gi·ªõi thi·ªáu
    st.markdown("""
    <div style='background: linear-gradient(135deg, #e3f2fd, #f3e5f5); padding: 2rem; border-radius: 15px; margin-bottom: 2rem;'>
        <h3 style='color: #1565c0; text-align: center;'>üéØ C√îNG C·ª§ H·ªåC T·∫¨P V·ªöI WEB SCRAPING M·∫†NH M·∫º</h3>
        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 1rem; text-align: center; margin-top: 1.5rem;">
            <div>
                <h4 style="color: #1976d2;">üåê Web Scraping</h4>
                <p>Thu th·∫≠p d·ªØ li·ªáu t·ª´ web phong ph√∫</p>
            </div>
            <div>
                <h4 style="color: #1976d2;">ü§ù Collocation</h4>
                <p>H·ªçc t·ª´ theo c·ª•m t·ª± nhi√™n</p>
            </div>
            <div>
                <h4 style="color: #1976d2;">üìö 240 t·ª´ AWL</h4>
                <p>T·ª´ v·ª±ng h·ªçc thu·∫≠t c·ªët l√µi</p>
            </div>
            <div>
                <h4 style="color: #1976d2;">üé§ Gi·ªçng n√≥i</h4>
                <p>T√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i</p>
            </div>
        </div>
        <div style="margin-top: 1rem; text-align: center; font-size: 0.9rem; color: #546e7a;">
            üåê Web Scraping t·ª´: Google, Vtudien, Wiktionary, v√† c√°c ngu·ªìn ti·∫øng Vi·ªát
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # Th√¥ng tin ngu·ªìn d·ªØ li·ªáu
    with st.expander("üìä TH√îNG TIN NGU·ªíN D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C S·ª¨ D·ª§NG"):
        st.markdown("""
        ### üéØ C√ÅC NGU·ªíN D·ªÆ LI·ªÜU CH√çNH TH·ª®C & WEB SCRAPING
        
        **üåê API C·ªë ƒê·ªãnh:**
        - **Free Dictionary API**: API mi·ªÖn ph√≠, ·ªïn ƒë·ªãnh ~99%
        - **Google Translate API**: D·ªãch thu·∫≠t ch√≠nh x√°c
        
        **üíæ Database H·ªçc Thu·∫≠t:**
        - **WordNet Database**: Database t·ª´ v·ª±ng h·ªçc thu·∫≠t t·ª´ Princeton University
        - **Academic Word List**: 240 t·ª´ v·ª±ng h·ªçc thu·∫≠t quan tr·ªçng nh·∫•t
        
        **üåê WEB SCRAPING Sources:**
        - **Google Search**: T√¨m ki·∫øm v√≠ d·ª•, t·ª´ ƒë·ªìng nghƒ©a, th√†nh ng·ªØ
        - **Vtudien.com**: ƒê·ªãnh nghƒ©a ti·∫øng Vi·ªát ch√≠nh x√°c
        - **Wiktionary**: T·ª´ nguy√™n, ƒë·ªãnh nghƒ©a ƒëa ng√¥n ng·ªØ
        - **C√°c trang web ti·∫øng Vi·ªát**: Thu th·∫≠p d·ªØ li·ªáu phong ph√∫
        
        **üìà D·ªØ li·ªáu Ph√¢n t√≠ch:**
        - **Word Frequency Database**: T·∫ßn su·∫•t s·ª≠ d·ª•ng t·ª´ theo khung CEFR
        - **Context Examples Database**: V√≠ d·ª• ng·ªØ c·∫£nh t·ª´ s√°ch gi√°o khoa
        
        **üé§ C√¥ng ngh·ªá Gi·ªçng n√≥i:**
        - **SpeechRecognition**: Nh·∫≠n di·ªán gi·ªçng n√≥i ƒëa ng√¥n ng·ªØ
        - **Google Speech API**: H·ªó tr·ª£ ti·∫øng Anh v√† ti·∫øng Vi·ªát
        
        **‚úÖ ƒê·∫∂C ƒêI·ªÜM:**
        - **Web Scraping** cho ti·∫øng Vi·ªát
        - D·ªØ li·ªáu chu·∫©n h·ªçc thu·∫≠t
        - Ph√π h·ª£p thi ƒë√°nh gi√° nƒÉng l·ª±c
        - Database n·ªôi b·ªô phong ph√∫
        - H·ªó tr·ª£ t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i (upload file audio)
        """)
    
    # Tab ch·ª©c nƒÉng ch√≠nh
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìö 240 T·ª™ H·ªåC THU·∫¨T", 
        "üá∫üá∏ ANH-VI·ªÜT", 
        "üáªüá≥ VI·ªÜT-ANH", 
        "üî§ VI·ªÜT-VI·ªÜT"
    ])
    
    with tab1:
        display_academic_words_section()
    
    with tab2:
        display_english_vietnamese_advanced()
    
    with tab3:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        display_vietnamese_english()
        st.markdown('</div>', unsafe_allow_html=True)
    
    with tab4:
        st.markdown('<div class="tab-content">', unsafe_allow_html=True)
        display_vietnamese_vietnamese()
        st.markdown('</div>', unsafe_allow_html=True)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #546e7a;'>"
        "üè´ <strong>V·ªû GHI ƒêI·ªÜN T·ª¨ H·ªñ TR·ª¢ H·ªåC T·ª™ V·ª∞NG SONG NG·ªÆ ANH - VI·ªÜT<br>"
        "üìö 240 t·ª´ v·ª±ng h·ªçc thu·∫≠t AWL | üåê Web Scraping m·∫°nh m·∫Ω | ü§ù Collocation h·ªçc thu·∫≠t<br>"
        "H·ªó tr·ª£ t√¨m ki·∫øm b·∫±ng gi·ªçng n√≥i | Ph·ª•c v·ª• √¥n thi ƒêGNL - ƒêGTD c√°c tr∆∞·ªùng ƒê·∫°i h·ªçc<br>"
        "¬© 2024 - Phi√™n b·∫£n ho√†n ch·ªânh v·ªõi Web Scraping cho Streamlit Cloud"
        "</div>",
        unsafe_allow_html=True
    )

# ==================== INITIALIZE ====================

# Kh·ªüi t·∫°o voice search
voice_search = VoiceSearchSimple()

if __name__ == "__main__":
    # Kh·ªüi t·∫°o session state
    if "advanced_en_input" not in st.session_state:
        st.session_state.advanced_en_input = ""
    if "vi_en_input" not in st.session_state:
        st.session_state.vi_en_input = ""
    if "vi_vi_input" not in st.session_state:
        st.session_state.vi_vi_input = ""
    if "last_searched" not in st.session_state:
        st.session_state.last_searched = ""
    if "selected_academic_word" not in st.session_state:
        st.session_state.selected_academic_word = ""
    
    # Ki·ªÉm tra n·∫øu c√≥ t·ª´ ƒë∆∞·ª£c ch·ªçn t·ª´ danh s√°ch h·ªçc thu·∫≠t
    try:
        import streamlit as st
        # JavaScript ƒë√£ x·ª≠ l√Ω vi·ªác l∆∞u t·ª´ v√†o session storage
        # ·ªû ƒë√¢y ta ch·ªâ c·∫ßn ki·ªÉm tra v√† x·ª≠ l√Ω
        if st.session_state.get("selected_academic_word"):
            st.session_state.advanced_en_input = st.session_state.selected_academic_word
    except:
        pass
    
    main()